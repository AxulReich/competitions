{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Part 1: For Beginners - Bag of Words\n",
    "\n",
    "[source](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words)\n",
    "\n",
    "## Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_table('data/labeledTrainData.tsv', \n",
    "                      delimiter = '\\t', \n",
    "                      quoting = 3)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>\"A very accurate depiction of small time mob l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             review\n",
       "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
       "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
       "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
       "3    \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
       "4   \"12128_7\"  \"A very accurate depiction of small time mob l..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_table('data/testData.tsv', \n",
    "                     delimiter = '\\t', \n",
    "                     quoting = 3)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"9999_0\"</td>\n",
       "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"45057_0\"</td>\n",
       "      <td>\"I saw this film about 20 years ago and rememb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"15561_0\"</td>\n",
       "      <td>\"Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7161_0\"</td>\n",
       "      <td>\"I went to see this film with a great deal of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"43971_0\"</td>\n",
       "      <td>\"Yes, I agree with everyone on this site this ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                             review\n",
       "0   \"9999_0\"  \"Watching Time Chasers, it obvious that it was...\n",
       "1  \"45057_0\"  \"I saw this film about 20 years ago and rememb...\n",
       "2  \"15561_0\"  \"Minor Spoilers<br /><br />In New York, Joan B...\n",
       "3   \"7161_0\"  \"I went to see this film with a great deal of ...\n",
       "4  \"43971_0\"  \"Yes, I agree with everyone on this site this ..."
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_train = pd.read_csv(\"data/unlabeledTrainData.tsv\", \n",
    "                              delimiter = \"\\t\", \n",
    "                              quoting = 3)\n",
    "\n",
    "unlabeled_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## Data Cleaning and Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words(raw_review):\n",
    "    \"\"\"Function to convert a raw review to a string of words\n",
    "    The input is a single string (a raw movie review), and \n",
    "    the output is a single string (a preprocessed movie review)\n",
    "    \"\"\"\n",
    "\n",
    "    review_text = BeautifulSoup(raw_review, 'lxml').get_text() \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    words = letters_only.lower().split()                             \n",
    "    \n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "\n",
    "    return(\" \".join( meaningful_words ))   \n",
    "\n",
    "# review_to_words( train[\"review\"][0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.4 s, sys: 1.76 s, total: 28.2 s\n",
      "Wall time: 29.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clean_train_reviews = [review_to_words(review) for review in train.review]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## Creating Features from a Bag of Words (Using `scikit-learn`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             max_features = 5000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.09 s, sys: 117 ms, total: 4.2 s\n",
      "Wall time: 4.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_features = train_data_features.toarray()\n",
    "# print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "# print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're interested, you can also print the counts of each word in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Sum up the counts of each vocabulary word\n",
    "# dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# # For each, print the vocabulary word and the number of times it \n",
    "# # appears in the training set\n",
    "# for tag, count in zip(vocab, dist):\n",
    "#     print(count, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Random Forest\n",
    "\n",
    "Initializing a Random Forest classifier with 100 trees and fitting the forest to the training set, using the bag of words as features and the sentiment labels as the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 10s, sys: 4.06 s, total: 2min 14s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "forest = forest.fit(train_data_features, train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Making Predictions\n",
    "\n",
    "Doing the same stuff, but with the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.5 s, sys: 1.94 s, total: 28.4 s\n",
      "Wall time: 30.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clean_test_reviews = [review_to_words(review) for review in test.review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.78 s, sys: 225 ms, total: 5 s\n",
      "Wall time: 5.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.87 s, sys: 1.73 s, total: 4.61 s\n",
      "Wall time: 6.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = forest.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Creating a Submission\n",
    "# output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "# output.to_csv(\"submissions/Bag_of_Words_model.csv\", \n",
    "#               index = False, \n",
    "#               quoting = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "# Part 2: Word Vectors\n",
    "\n",
    "[source](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords = False):\n",
    "    \"\"\"Function to convert a document to a sequence of words,\n",
    "    optionally removing stop words.\n",
    "    Returns a list of words.\n",
    "    \"\"\"\n",
    "    review_text = BeautifulSoup(review, 'lxml').get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    \n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `punkt` tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_sentences(review, tokenizer, remove_stopwords = False):\n",
    "    \"\"\"Function to split a review into parsed sentences.\n",
    "    Returns a list of sentences, \n",
    "    where each sentence is a list of words\n",
    "    \"\"\"\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    \n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    \n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    \n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 27s, sys: 8.11 s, total: 2min 35s\n",
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentences = []\n",
    "\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.5/site-packages/bs4/__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 42s, sys: 20.1 s, total: 5min 2s\n",
      "Wall time: 5min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Training and Saving Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format = '%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level = logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and train the model.\n",
    "\n",
    "This will take some time: with `num_workers = 4` \n",
    "```\n",
    "CPU times: user 8min 20s, sys: 13.3 s, total: 8min 33s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-11-10 22:19:00,277 : INFO : collecting all words and their counts\n",
      "2016-11-10 22:19:00,281 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2016-11-10 22:19:00,431 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n",
      "2016-11-10 22:19:00,531 : INFO : PROGRESS: at sentence #20000, processed 451867 words, keeping 24947 word types\n",
      "2016-11-10 22:19:00,629 : INFO : PROGRESS: at sentence #30000, processed 671290 words, keeping 30033 word types\n",
      "2016-11-10 22:19:00,727 : INFO : PROGRESS: at sentence #40000, processed 897790 words, keeping 34347 word types\n",
      "2016-11-10 22:19:00,822 : INFO : PROGRESS: at sentence #50000, processed 1116929 words, keeping 37760 word types\n",
      "2016-11-10 22:19:00,919 : INFO : PROGRESS: at sentence #60000, processed 1338370 words, keeping 40722 word types\n",
      "2016-11-10 22:19:01,019 : INFO : PROGRESS: at sentence #70000, processed 1561505 words, keeping 43332 word types\n",
      "2016-11-10 22:19:01,116 : INFO : PROGRESS: at sentence #80000, processed 1780812 words, keeping 45713 word types\n",
      "2016-11-10 22:19:01,218 : INFO : PROGRESS: at sentence #90000, processed 2004905 words, keeping 48134 word types\n",
      "2016-11-10 22:19:01,338 : INFO : PROGRESS: at sentence #100000, processed 2226863 words, keeping 50206 word types\n",
      "2016-11-10 22:19:01,437 : INFO : PROGRESS: at sentence #110000, processed 2446412 words, keeping 52077 word types\n",
      "2016-11-10 22:19:01,539 : INFO : PROGRESS: at sentence #120000, processed 2668549 words, keeping 54113 word types\n",
      "2016-11-10 22:19:01,645 : INFO : PROGRESS: at sentence #130000, processed 2894077 words, keeping 55841 word types\n",
      "2016-11-10 22:19:01,741 : INFO : PROGRESS: at sentence #140000, processed 3106779 words, keeping 57340 word types\n",
      "2016-11-10 22:19:01,846 : INFO : PROGRESS: at sentence #150000, processed 3332313 words, keeping 59049 word types\n",
      "2016-11-10 22:19:01,946 : INFO : PROGRESS: at sentence #160000, processed 3555001 words, keeping 60611 word types\n",
      "2016-11-10 22:19:02,047 : INFO : PROGRESS: at sentence #170000, processed 3778341 words, keeping 62071 word types\n",
      "2016-11-10 22:19:02,152 : INFO : PROGRESS: at sentence #180000, processed 3998922 words, keeping 63490 word types\n",
      "2016-11-10 22:19:02,257 : INFO : PROGRESS: at sentence #190000, processed 4224105 words, keeping 64788 word types\n",
      "2016-11-10 22:19:02,371 : INFO : PROGRESS: at sentence #200000, processed 4448225 words, keeping 66079 word types\n",
      "2016-11-10 22:19:02,481 : INFO : PROGRESS: at sentence #210000, processed 4669555 words, keeping 67383 word types\n",
      "2016-11-10 22:19:02,596 : INFO : PROGRESS: at sentence #220000, processed 4894556 words, keeping 68690 word types\n",
      "2016-11-10 22:19:02,704 : INFO : PROGRESS: at sentence #230000, processed 5117022 words, keeping 69950 word types\n",
      "2016-11-10 22:19:02,819 : INFO : PROGRESS: at sentence #240000, processed 5344527 words, keeping 71159 word types\n",
      "2016-11-10 22:19:02,929 : INFO : PROGRESS: at sentence #250000, processed 5558635 words, keeping 72343 word types\n",
      "2016-11-10 22:19:03,061 : INFO : PROGRESS: at sentence #260000, processed 5778616 words, keeping 73470 word types\n",
      "2016-11-10 22:19:03,167 : INFO : PROGRESS: at sentence #270000, processed 5999905 words, keeping 74759 word types\n",
      "2016-11-10 22:19:03,278 : INFO : PROGRESS: at sentence #280000, processed 6225784 words, keeping 76361 word types\n",
      "2016-11-10 22:19:03,387 : INFO : PROGRESS: at sentence #290000, processed 6448944 words, keeping 77831 word types\n",
      "2016-11-10 22:19:03,501 : INFO : PROGRESS: at sentence #300000, processed 6673547 words, keeping 79163 word types\n",
      "2016-11-10 22:19:03,615 : INFO : PROGRESS: at sentence #310000, processed 6898861 words, keeping 80472 word types\n",
      "2016-11-10 22:19:03,736 : INFO : PROGRESS: at sentence #320000, processed 7123745 words, keeping 81800 word types\n",
      "2016-11-10 22:19:03,840 : INFO : PROGRESS: at sentence #330000, processed 7345404 words, keeping 83022 word types\n",
      "2016-11-10 22:19:03,952 : INFO : PROGRESS: at sentence #340000, processed 7574852 words, keeping 84272 word types\n",
      "2016-11-10 22:19:04,061 : INFO : PROGRESS: at sentence #350000, processed 7798104 words, keeping 85417 word types\n",
      "2016-11-10 22:19:04,174 : INFO : PROGRESS: at sentence #360000, processed 8018679 words, keeping 86587 word types\n",
      "2016-11-10 22:19:04,299 : INFO : PROGRESS: at sentence #370000, processed 8245856 words, keeping 87699 word types\n",
      "2016-11-10 22:19:04,423 : INFO : PROGRESS: at sentence #380000, processed 8470979 words, keeping 88869 word types\n",
      "2016-11-10 22:19:04,548 : INFO : PROGRESS: at sentence #390000, processed 8700551 words, keeping 89893 word types\n",
      "2016-11-10 22:19:04,662 : INFO : PROGRESS: at sentence #400000, processed 8923483 words, keeping 90902 word types\n",
      "2016-11-10 22:19:04,781 : INFO : PROGRESS: at sentence #410000, processed 9144826 words, keeping 91866 word types\n",
      "2016-11-10 22:19:04,895 : INFO : PROGRESS: at sentence #420000, processed 9365858 words, keeping 92899 word types\n",
      "2016-11-10 22:19:05,009 : INFO : PROGRESS: at sentence #430000, processed 9593338 words, keeping 93919 word types\n",
      "2016-11-10 22:19:05,137 : INFO : PROGRESS: at sentence #440000, processed 9820034 words, keeping 94893 word types\n",
      "2016-11-10 22:19:05,254 : INFO : PROGRESS: at sentence #450000, processed 10043796 words, keeping 96024 word types\n",
      "2016-11-10 22:19:05,370 : INFO : PROGRESS: at sentence #460000, processed 10276556 words, keeping 97076 word types\n",
      "2016-11-10 22:19:05,481 : INFO : PROGRESS: at sentence #470000, processed 10504440 words, keeping 97921 word types\n",
      "2016-11-10 22:19:05,594 : INFO : PROGRESS: at sentence #480000, processed 10724824 words, keeping 98850 word types\n",
      "2016-11-10 22:19:05,713 : INFO : PROGRESS: at sentence #490000, processed 10951484 words, keeping 99858 word types\n",
      "2016-11-10 22:19:05,839 : INFO : PROGRESS: at sentence #500000, processed 11173140 words, keeping 100752 word types\n",
      "2016-11-10 22:19:06,096 : INFO : PROGRESS: at sentence #510000, processed 11398393 words, keeping 101685 word types\n",
      "2016-11-10 22:19:06,213 : INFO : PROGRESS: at sentence #520000, processed 11621731 words, keeping 102584 word types\n",
      "2016-11-10 22:19:06,338 : INFO : PROGRESS: at sentence #530000, processed 11846081 words, keeping 103386 word types\n",
      "2016-11-10 22:19:06,459 : INFO : PROGRESS: at sentence #540000, processed 12070696 words, keeping 104251 word types\n",
      "2016-11-10 22:19:06,622 : INFO : PROGRESS: at sentence #550000, processed 12296195 words, keeping 105117 word types\n",
      "2016-11-10 22:19:06,778 : INFO : PROGRESS: at sentence #560000, processed 12517476 words, keeping 105981 word types\n",
      "2016-11-10 22:19:06,909 : INFO : PROGRESS: at sentence #570000, processed 12746461 words, keeping 106771 word types\n",
      "2016-11-10 22:19:07,037 : INFO : PROGRESS: at sentence #580000, processed 12967949 words, keeping 107650 word types\n",
      "2016-11-10 22:19:07,153 : INFO : PROGRESS: at sentence #590000, processed 13193474 words, keeping 108486 word types\n",
      "2016-11-10 22:19:07,270 : INFO : PROGRESS: at sentence #600000, processed 13415660 words, keeping 109203 word types\n",
      "2016-11-10 22:19:07,409 : INFO : PROGRESS: at sentence #610000, processed 13636656 words, keeping 110077 word types\n",
      "2016-11-10 22:19:07,538 : INFO : PROGRESS: at sentence #620000, processed 13862938 words, keeping 110822 word types\n",
      "2016-11-10 22:19:07,660 : INFO : PROGRESS: at sentence #630000, processed 14087222 words, keeping 111596 word types\n",
      "2016-11-10 22:19:07,787 : INFO : PROGRESS: at sentence #640000, processed 14307942 words, keeping 112402 word types\n",
      "2016-11-10 22:19:07,906 : INFO : PROGRESS: at sentence #650000, processed 14533624 words, keeping 113182 word types\n",
      "2016-11-10 22:19:08,048 : INFO : PROGRESS: at sentence #660000, processed 14756308 words, keeping 113932 word types\n",
      "2016-11-10 22:19:08,174 : INFO : PROGRESS: at sentence #670000, processed 14979692 words, keeping 114630 word types\n",
      "2016-11-10 22:19:08,287 : INFO : PROGRESS: at sentence #680000, processed 15204499 words, keeping 115341 word types\n",
      "2016-11-10 22:19:08,407 : INFO : PROGRESS: at sentence #690000, processed 15426666 words, keeping 116118 word types\n",
      "2016-11-10 22:19:08,539 : INFO : PROGRESS: at sentence #700000, processed 15655301 words, keeping 116930 word types\n",
      "2016-11-10 22:19:08,669 : INFO : PROGRESS: at sentence #710000, processed 15878274 words, keeping 117583 word types\n",
      "2016-11-10 22:19:08,795 : INFO : PROGRESS: at sentence #720000, processed 16103516 words, keeping 118207 word types\n",
      "2016-11-10 22:19:08,916 : INFO : PROGRESS: at sentence #730000, processed 16329897 words, keeping 118940 word types\n",
      "2016-11-10 22:19:09,034 : INFO : PROGRESS: at sentence #740000, processed 16550913 words, keeping 119654 word types\n",
      "2016-11-10 22:19:09,145 : INFO : PROGRESS: at sentence #750000, processed 16769240 words, keeping 120282 word types\n",
      "2016-11-10 22:19:09,264 : INFO : PROGRESS: at sentence #760000, processed 16988632 words, keeping 120917 word types\n",
      "2016-11-10 22:19:09,389 : INFO : PROGRESS: at sentence #770000, processed 17215761 words, keeping 121690 word types\n",
      "2016-11-10 22:19:09,525 : INFO : PROGRESS: at sentence #780000, processed 17445902 words, keeping 122389 word types\n",
      "2016-11-10 22:19:09,872 : INFO : PROGRESS: at sentence #790000, processed 17672895 words, keeping 123055 word types\n",
      "2016-11-10 22:19:10,087 : INFO : collected 123493 word types from a corpus of 17795898 raw words and 795538 sentences\n",
      "2016-11-10 22:19:10,907 : INFO : min_count=40 retains 16490 unique words (drops 107003)\n",
      "2016-11-10 22:19:10,909 : INFO : min_count leaves 17236863 word corpus (96% of original 17795898)\n",
      "2016-11-10 22:19:11,040 : INFO : deleting the raw counts dictionary of 123493 items\n",
      "2016-11-10 22:19:11,054 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2016-11-10 22:19:11,056 : INFO : downsampling leaves estimated 12748070 word corpus (74.0% of prior 17236863)\n",
      "2016-11-10 22:19:11,059 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2016-11-10 22:19:11,184 : INFO : resetting layer weights\n",
      "2016-11-10 22:19:12,049 : INFO : training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5\n",
      "2016-11-10 22:19:12,051 : INFO : expecting 795538 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-11-10 22:19:13,104 : INFO : PROGRESS: at 0.56% examples, 354008 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:14,118 : INFO : PROGRESS: at 1.31% examples, 413092 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:15,120 : INFO : PROGRESS: at 2.06% examples, 432496 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:16,137 : INFO : PROGRESS: at 2.80% examples, 438672 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 22:19:17,167 : INFO : PROGRESS: at 3.56% examples, 444105 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:18,233 : INFO : PROGRESS: at 4.27% examples, 440393 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:19,243 : INFO : PROGRESS: at 4.91% examples, 435228 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:20,262 : INFO : PROGRESS: at 5.62% examples, 436039 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:21,311 : INFO : PROGRESS: at 6.30% examples, 432921 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:22,321 : INFO : PROGRESS: at 6.70% examples, 414601 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:19:23,347 : INFO : PROGRESS: at 7.12% examples, 400922 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:24,363 : INFO : PROGRESS: at 7.61% examples, 393388 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:25,403 : INFO : PROGRESS: at 8.21% examples, 391101 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:26,413 : INFO : PROGRESS: at 8.83% examples, 391466 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:27,436 : INFO : PROGRESS: at 9.42% examples, 390016 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:19:28,437 : INFO : PROGRESS: at 10.00% examples, 388904 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:29,451 : INFO : PROGRESS: at 10.61% examples, 388421 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:30,486 : INFO : PROGRESS: at 11.27% examples, 389474 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:31,520 : INFO : PROGRESS: at 11.89% examples, 389730 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:32,535 : INFO : PROGRESS: at 12.55% examples, 391009 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 22:19:33,541 : INFO : PROGRESS: at 13.21% examples, 392342 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:34,548 : INFO : PROGRESS: at 13.84% examples, 392583 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:35,564 : INFO : PROGRESS: at 14.35% examples, 389613 words/s, in_qsize 5, out_qsize 2\n",
      "2016-11-10 22:19:36,574 : INFO : PROGRESS: at 14.84% examples, 386083 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:37,586 : INFO : PROGRESS: at 15.54% examples, 388459 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:38,594 : INFO : PROGRESS: at 16.28% examples, 391226 words/s, in_qsize 7, out_qsize 1\n",
      "2016-11-10 22:19:39,718 : INFO : PROGRESS: at 16.95% examples, 390853 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 22:19:40,744 : INFO : PROGRESS: at 17.34% examples, 385585 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:41,756 : INFO : PROGRESS: at 17.92% examples, 384965 words/s, in_qsize 8, out_qsize 1\n",
      "2016-11-10 22:19:42,771 : INFO : PROGRESS: at 18.54% examples, 385051 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:43,776 : INFO : PROGRESS: at 19.25% examples, 387055 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:44,782 : INFO : PROGRESS: at 19.92% examples, 388476 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:45,784 : INFO : PROGRESS: at 20.61% examples, 389865 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:19:46,801 : INFO : PROGRESS: at 21.34% examples, 391827 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:47,804 : INFO : PROGRESS: at 22.07% examples, 393843 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:48,827 : INFO : PROGRESS: at 22.82% examples, 395734 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:49,831 : INFO : PROGRESS: at 23.54% examples, 397138 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:50,837 : INFO : PROGRESS: at 24.26% examples, 398643 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:51,838 : INFO : PROGRESS: at 24.99% examples, 400307 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:19:52,855 : INFO : PROGRESS: at 25.63% examples, 400303 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:19:53,874 : INFO : PROGRESS: at 26.39% examples, 401984 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:19:54,882 : INFO : PROGRESS: at 27.16% examples, 403861 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:55,887 : INFO : PROGRESS: at 27.91% examples, 405537 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:56,890 : INFO : PROGRESS: at 28.65% examples, 407156 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:19:57,915 : INFO : PROGRESS: at 29.42% examples, 408813 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:19:58,929 : INFO : PROGRESS: at 30.06% examples, 408681 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:19:59,934 : INFO : PROGRESS: at 30.71% examples, 408754 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:20:00,941 : INFO : PROGRESS: at 31.43% examples, 409823 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:01,952 : INFO : PROGRESS: at 32.17% examples, 411118 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:02,962 : INFO : PROGRESS: at 32.95% examples, 412665 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:03,969 : INFO : PROGRESS: at 33.71% examples, 414022 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:20:04,977 : INFO : PROGRESS: at 34.47% examples, 415337 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:05,984 : INFO : PROGRESS: at 35.23% examples, 416607 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:20:07,004 : INFO : PROGRESS: at 35.92% examples, 416809 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:08,012 : INFO : PROGRESS: at 36.60% examples, 417086 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:09,018 : INFO : PROGRESS: at 37.12% examples, 415506 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:20:10,038 : INFO : PROGRESS: at 37.39% examples, 411145 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:11,083 : INFO : PROGRESS: at 37.85% examples, 408953 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:12,089 : INFO : PROGRESS: at 38.30% examples, 406871 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:13,113 : INFO : PROGRESS: at 38.61% examples, 403319 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:14,138 : INFO : PROGRESS: at 38.94% examples, 399999 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:15,169 : INFO : PROGRESS: at 39.57% examples, 399806 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:16,198 : INFO : PROGRESS: at 40.24% examples, 400077 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 22:20:17,240 : INFO : PROGRESS: at 40.73% examples, 398506 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 22:20:18,286 : INFO : PROGRESS: at 41.13% examples, 396094 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:19,289 : INFO : PROGRESS: at 41.81% examples, 396562 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:20,301 : INFO : PROGRESS: at 42.49% examples, 396978 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:21,307 : INFO : PROGRESS: at 43.17% examples, 397415 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:22,345 : INFO : PROGRESS: at 43.88% examples, 397846 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 22:20:23,358 : INFO : PROGRESS: at 44.55% examples, 398216 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:20:24,360 : INFO : PROGRESS: at 45.23% examples, 398637 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:25,375 : INFO : PROGRESS: at 45.90% examples, 398967 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:26,378 : INFO : PROGRESS: at 46.57% examples, 399245 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 22:20:27,384 : INFO : PROGRESS: at 47.27% examples, 399792 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:28,395 : INFO : PROGRESS: at 47.94% examples, 400117 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:29,402 : INFO : PROGRESS: at 48.58% examples, 400271 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:20:30,407 : INFO : PROGRESS: at 49.23% examples, 400427 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:31,432 : INFO : PROGRESS: at 49.85% examples, 400303 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:32,432 : INFO : PROGRESS: at 50.52% examples, 400571 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:33,438 : INFO : PROGRESS: at 50.96% examples, 399127 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:34,497 : INFO : PROGRESS: at 51.45% examples, 397809 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:35,500 : INFO : PROGRESS: at 52.08% examples, 397905 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:20:36,505 : INFO : PROGRESS: at 52.81% examples, 398673 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:37,514 : INFO : PROGRESS: at 53.50% examples, 399153 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:38,534 : INFO : PROGRESS: at 54.19% examples, 399501 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:39,770 : INFO : PROGRESS: at 54.88% examples, 398924 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:40,787 : INFO : PROGRESS: at 55.24% examples, 396928 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:41,843 : INFO : PROGRESS: at 55.71% examples, 395613 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:42,869 : INFO : PROGRESS: at 56.15% examples, 394216 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:20:43,871 : INFO : PROGRESS: at 56.56% examples, 392715 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:20:44,877 : INFO : PROGRESS: at 57.05% examples, 391846 words/s, in_qsize 5, out_qsize 2\n",
      "2016-11-10 22:20:45,881 : INFO : PROGRESS: at 57.53% examples, 390934 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:20:46,883 : INFO : PROGRESS: at 58.20% examples, 391323 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:47,912 : INFO : PROGRESS: at 58.91% examples, 391818 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 22:20:48,913 : INFO : PROGRESS: at 59.62% examples, 392498 words/s, in_qsize 7, out_qsize 1\n",
      "2016-11-10 22:20:49,916 : INFO : PROGRESS: at 60.35% examples, 393223 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:50,919 : INFO : PROGRESS: at 61.07% examples, 393933 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:20:51,927 : INFO : PROGRESS: at 61.80% examples, 394537 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:52,928 : INFO : PROGRESS: at 62.53% examples, 395167 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:53,938 : INFO : PROGRESS: at 63.25% examples, 395747 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:54,940 : INFO : PROGRESS: at 63.98% examples, 396333 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:55,942 : INFO : PROGRESS: at 64.70% examples, 396919 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:56,943 : INFO : PROGRESS: at 65.40% examples, 397350 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:57,967 : INFO : PROGRESS: at 66.11% examples, 397829 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:20:58,973 : INFO : PROGRESS: at 66.83% examples, 398301 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:20:59,979 : INFO : PROGRESS: at 67.53% examples, 398692 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:00,993 : INFO : PROGRESS: at 68.24% examples, 399180 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:02,000 : INFO : PROGRESS: at 68.92% examples, 399496 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:21:03,014 : INFO : PROGRESS: at 69.60% examples, 399767 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:04,019 : INFO : PROGRESS: at 70.30% examples, 400145 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:05,020 : INFO : PROGRESS: at 70.92% examples, 400144 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:21:06,026 : INFO : PROGRESS: at 71.59% examples, 400432 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:07,037 : INFO : PROGRESS: at 72.27% examples, 400704 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:08,049 : INFO : PROGRESS: at 72.96% examples, 400974 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:09,068 : INFO : PROGRESS: at 73.65% examples, 401267 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:10,111 : INFO : PROGRESS: at 74.11% examples, 400209 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:11,133 : INFO : PROGRESS: at 74.69% examples, 399897 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:12,134 : INFO : PROGRESS: at 75.28% examples, 399659 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:13,159 : INFO : PROGRESS: at 75.96% examples, 399879 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:21:14,164 : INFO : PROGRESS: at 76.71% examples, 400513 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:15,177 : INFO : PROGRESS: at 77.45% examples, 401049 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:16,180 : INFO : PROGRESS: at 78.16% examples, 401437 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:21:17,187 : INFO : PROGRESS: at 78.89% examples, 401925 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:21:18,189 : INFO : PROGRESS: at 79.62% examples, 402473 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:19,197 : INFO : PROGRESS: at 80.36% examples, 402987 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:20,205 : INFO : PROGRESS: at 81.10% examples, 403503 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:21,206 : INFO : PROGRESS: at 81.85% examples, 404027 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:22,207 : INFO : PROGRESS: at 82.59% examples, 404495 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:23,242 : INFO : PROGRESS: at 83.35% examples, 404952 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:24,246 : INFO : PROGRESS: at 84.10% examples, 405498 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:21:25,258 : INFO : PROGRESS: at 84.86% examples, 406016 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:26,265 : INFO : PROGRESS: at 85.58% examples, 406429 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:27,267 : INFO : PROGRESS: at 86.34% examples, 406902 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:21:28,280 : INFO : PROGRESS: at 87.09% examples, 407393 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:21:29,287 : INFO : PROGRESS: at 87.84% examples, 407897 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:30,308 : INFO : PROGRESS: at 88.58% examples, 408347 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:31,309 : INFO : PROGRESS: at 89.32% examples, 408797 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:32,337 : INFO : PROGRESS: at 90.06% examples, 409172 words/s, in_qsize 6, out_qsize 3\n",
      "2016-11-10 22:21:33,345 : INFO : PROGRESS: at 90.83% examples, 409742 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:34,360 : INFO : PROGRESS: at 91.57% examples, 410180 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:21:35,361 : INFO : PROGRESS: at 92.32% examples, 410664 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:36,370 : INFO : PROGRESS: at 93.08% examples, 411164 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:21:37,382 : INFO : PROGRESS: at 93.83% examples, 411599 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:21:38,396 : INFO : PROGRESS: at 94.56% examples, 411925 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:39,399 : INFO : PROGRESS: at 95.29% examples, 412276 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:40,416 : INFO : PROGRESS: at 95.74% examples, 411378 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 22:21:41,421 : INFO : PROGRESS: at 96.36% examples, 411291 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:21:42,438 : INFO : PROGRESS: at 97.02% examples, 411315 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 22:21:43,439 : INFO : PROGRESS: at 97.67% examples, 411334 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:21:44,460 : INFO : PROGRESS: at 98.30% examples, 411204 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:21:45,478 : INFO : PROGRESS: at 99.05% examples, 411549 words/s, in_qsize 7, out_qsize 1\n",
      "2016-11-10 22:21:46,495 : INFO : PROGRESS: at 99.80% examples, 411985 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 22:21:46,730 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2016-11-10 22:21:46,742 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-11-10 22:21:46,747 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-11-10 22:21:46,771 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-11-10 22:21:46,773 : INFO : training on 88979490 raw words (63739967 effective words) took 154.7s, 412071 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 20s, sys: 13.3 s, total: 8min 33s\n",
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = word2vec.Word2Vec(sentences, \n",
    "                          workers = num_workers, \n",
    "                          size = num_features, \n",
    "                          min_count = min_word_count,\n",
    "                          window = context, \n",
    "                          sample = downsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't plan to train the model any further, calling `init_sims` will make the model much more memory-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-11-10 22:22:11,225 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "model.init_sims(replace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be helpful to create a meaningful model name and save the model for later use. You can load it later using `Word2Vec.load()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-11-10 22:22:13,391 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2016-11-10 22:22:13,444 : INFO : not storing attribute syn0norm\n",
      "2016-11-10 22:22:13,446 : INFO : not storing attribute cum_table\n"
     ]
    }
   ],
   "source": [
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6106343865394592),\n",
       " ('monk', 0.5867377519607544),\n",
       " ('lady', 0.585590124130249),\n",
       " ('lad', 0.573781430721283),\n",
       " ('soldier', 0.5376678705215454),\n",
       " ('businessman', 0.5143798589706421),\n",
       " ('farmer', 0.5093753337860107),\n",
       " ('men', 0.5063858032226562),\n",
       " ('guy', 0.5043983459472656),\n",
       " ('person', 0.5014356970787048)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bride', 0.6558289527893066),\n",
       " ('princess', 0.6477103233337402),\n",
       " ('latifah', 0.6186422109603882),\n",
       " ('goddess', 0.6099314093589783),\n",
       " ('stepmother', 0.5945958495140076),\n",
       " ('eva', 0.5876355767250061),\n",
       " ('prince', 0.5868668556213379),\n",
       " ('victoria', 0.580491840839386),\n",
       " ('mistress', 0.5729061365127563),\n",
       " ('angela', 0.570813775062561)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7688319683074951),\n",
       " ('atrocious', 0.7546607255935669),\n",
       " ('abysmal', 0.7259805798530579),\n",
       " ('horrible', 0.7223121523857117),\n",
       " ('dreadful', 0.7169843912124634),\n",
       " ('horrendous', 0.7052638530731201),\n",
       " ('horrid', 0.6820660829544067),\n",
       " ('lousy', 0.6667404770851135),\n",
       " ('appalling', 0.6601595878601074),\n",
       " ('amateurish', 0.6331586837768555)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Part 3: More Fun With Word Vectors\n",
    "\n",
    "[source](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors)\n",
    "\n",
    "## Numeric Representations of Words\n",
    "\n",
    "The Word2Vec model trained in Part 2 consists of a feature vector for each word in the vocabulary, stored in a `numpy` array called \"`syn0`\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-11-10 22:38:41,912 : INFO : loading Word2Vec object from 300features_40minwords_10context\n",
      "2016-11-10 22:38:42,781 : INFO : setting ignored attribute syn0norm to None\n",
      "2016-11-10 22:38:42,783 : INFO : setting ignored attribute cum_table to None\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16490, 300)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.syn0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows in `syn0` is the number of words in the model's vocabulary, and the number of columns corresponds to the size of the feature vector, which we set in Part 2.  Setting the minimum word count to 40 gave us a total vocabulary of 16,492 words with 300 features apiece. Individual word vectors can be accessed in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model[\"flower\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## From Words To Paragraphs, Attempt 1: Vector Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"Function to average all of the word vectors in a given\n",
    "    paragraph\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,), dtype = \"float64\")\n",
    "    nwords = 0.\n",
    "\n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    " \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    \"\"\"Given a set of reviews (each one a list of words), \n",
    "    calculate the average feature vector \n",
    "    for each one and return a 2D numpy array \n",
    "    \"\"\"\n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features), dtype = \"float64\")\n",
    "    \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        # Print a status message every 1000th review\n",
    "        if counter % 1000. == 0.:\n",
    "            (\"Review %d of %d\" % (counter, len(reviews)))\n",
    "        \n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        # Increment the counter\n",
    "        counter = counter + 1.\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate average feature vectors for training and testing sets,\n",
    "using the functions we defined above. Notice that we now use stop word\n",
    "removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 16s, sys: 2.86 s, total: 1min 19s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clean_train_reviews = []\n",
    "\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append(review_to_wordlist(review, \n",
    "                                                  remove_stopwords = True))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 16s, sys: 10.4 s, total: 1min 27s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clean_test_reviews = []\n",
    "\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Next, use the average paragraph vectors to train a random forest. Note that, as in Part 1, we can only use the labeled training reviews to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# forest = forest.fit(trainDataVecs, train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I get an error here:\n",
    "```\n",
    "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# result = forest.predict(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Write the test results \n",
    "# output = pd.DataFrame(data = {\"id\":test[\"id\"], \"sentiment\":result})\n",
    "# output.to_csv(\"Word2Vec_AverageVectors.csv\", \n",
    "#               index = False, \n",
    "#               quoting = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## From Words to Paragraphs, Attempt 2: Clustering \n",
    "\n",
    "Word2Vec creates clusters of semantically related words, so another possible approach is to exploit the similarity of words within a cluster. Grouping vectors in this way is known as \"vector quantization.\" To accomplish this, we first need to find the centers of the word clusters, which we can do by using a clustering algorithm such as K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set \"k\" (`num_clusters`) to be 1/5th of the vocabulary size, or an average of 5 words per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectors = model.syn0\n",
    "num_clusters = word_vectors.shape[0] // 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans_clustering = KMeans(n_clusters = num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 57s, sys: 49.4 s, total: 15min 46s\n",
      "Wall time: 11min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Word / Index dictionary, mapping each vocabulary word to a cluster number                                                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_centroid_map = dict(zip( model.index2word, idx ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little abstract, so let's take a closer look at what our clusters contain. Your clusters may differ, as Word2Vec relies on a random number seed. Here is a loop that prints out the words for clusters 0 through 9:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['pumpkinhead', 'sematary']\n",
      "\n",
      "Cluster 1\n",
      "['lobby', 'storage']\n",
      "\n",
      "Cluster 2\n",
      "['mirren', 'lansbury', 'walters', 'helen', 'ellen', 'chloe']\n",
      "\n",
      "Cluster 3\n",
      "['individual', 'external']\n",
      "\n",
      "Cluster 4\n",
      "['blaine', 'halle', 'durante']\n",
      "\n",
      "Cluster 5\n",
      "['rogue', 'drunken', 'hapless', 'heavies', 'bumbling', 'misfit']\n",
      "\n",
      "Cluster 6\n",
      "['avoids']\n",
      "\n",
      "Cluster 7\n",
      "['charge', 'hostage', 'retirement', 'custody', 'temporarily', 'willingly']\n",
      "\n",
      "Cluster 8\n",
      "['records', 'stones', 'notes', 'albums', 'videos', 'recordings']\n",
      "\n",
      "Cluster 9\n",
      "['emperor', 'inland', 'empire']\n"
     ]
    }
   ],
   "source": [
    "for cluster in range(10):\n",
    "    print(\"\\nCluster %d\" % cluster)\n",
    "\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    for i in range(len(word_centroid_map.values())):\n",
    "        if( list(word_centroid_map.values())[i] == cluster ):\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a cluster (or \"centroid\") assignment for each word, and we can define a function to convert reviews into bags-of-centroids. This works just like Bag of Words but uses semantically related clusters instead of individual words.\n",
    "\n",
    "* The number of clusters is equal to the highest cluster index in the word / centroid map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(wordlist, word_centroid_map):\n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    \n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    \n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    \n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the training set reviews into bags of centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.2 s, sys: 1.59 s, total: 44.8 s\n",
      "Wall time: 47.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_centroids = np.zeros((train[\"review\"].size, num_clusters),\n",
    "                           dtype = \"float32\")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for test reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.1 s, sys: 888 ms, total: 41 s\n",
      "Wall time: 41.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_centroids = np.zeros((test[\"review\"].size, num_clusters), \n",
    "                          dtype = \"float32\")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Fit a random forest and extract predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 11s, sys: 1.13 s, total: 1min 12s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "forest = forest.fit(train_centroids, train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.93 s, sys: 431 ms, total: 2.36 s\n",
      "Wall time: 3.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = forest.predict(test_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Write the test results \n",
    "# output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "# output.to_csv(\"BagOfCentroids.csv\", \n",
    "#               index = False, \n",
    "#               quoting = 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
