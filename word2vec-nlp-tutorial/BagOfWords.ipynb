{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Part 1: For Beginners - Bag of Words\n",
    "\n",
    "[source](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words)\n",
    "\n",
    "## Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_table('data/labeledTrainData.tsv', \n",
    "                      delimiter = '\\t', \n",
    "                      quoting = 3)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>\"A very accurate depiction of small time mob l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             review\n",
       "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
       "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
       "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
       "3    \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
       "4   \"12128_7\"  \"A very accurate depiction of small time mob l..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_table('data/testData.tsv', \n",
    "                     delimiter = '\\t', \n",
    "                     quoting = 3)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"9999_0\"</td>\n",
       "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"45057_0\"</td>\n",
       "      <td>\"I saw this film about 20 years ago and rememb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"15561_0\"</td>\n",
       "      <td>\"Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7161_0\"</td>\n",
       "      <td>\"I went to see this film with a great deal of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"43971_0\"</td>\n",
       "      <td>\"Yes, I agree with everyone on this site this ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                             review\n",
       "0   \"9999_0\"  \"Watching Time Chasers, it obvious that it was...\n",
       "1  \"45057_0\"  \"I saw this film about 20 years ago and rememb...\n",
       "2  \"15561_0\"  \"Minor Spoilers<br /><br />In New York, Joan B...\n",
       "3   \"7161_0\"  \"I went to see this film with a great deal of ...\n",
       "4  \"43971_0\"  \"Yes, I agree with everyone on this site this ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_train = pd.read_csv(\"data/unlabeledTrainData.tsv\", \n",
    "                              delimiter = \"\\t\", \n",
    "                              quoting = 3)\n",
    "\n",
    "unlabeled_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## Data Cleaning and Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words(raw_review):\n",
    "    \"\"\"Function to convert a raw review to a string of words\n",
    "    The input is a single string (a raw movie review), and \n",
    "    the output is a single string (a preprocessed movie review)\n",
    "    \"\"\"\n",
    "\n",
    "    review_text = BeautifulSoup(raw_review, 'lxml').get_text() \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    words = letters_only.lower().split()                             \n",
    "    \n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "\n",
    "    return(\" \".join( meaningful_words ))   \n",
    "\n",
    "# review_to_words( train[\"review\"][0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 s, sys: 1.19 s, total: 25.2 s\n",
      "Wall time: 26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clean_train_reviews = [review_to_words(review) for review in train.review]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## Creating Features from a Bag of Words (Using `scikit-learn`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             max_features = 5000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.73 s, sys: 197 ms, total: 4.93 s\n",
      "Wall time: 5.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_features = train_data_features.toarray()\n",
    "# print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "# print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're interested, you can also print the counts of each word in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Sum up the counts of each vocabulary word\n",
    "# dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# # For each, print the vocabulary word and the number of times it \n",
    "# # appears in the training set\n",
    "# for tag, count in zip(vocab, dist):\n",
    "#     print(count, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Random Forest\n",
    "\n",
    "Initializing a Random Forest classifier with 100 trees and fitting the forest to the training set, using the bag of words as features and the sentiment labels as the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 6s, sys: 3.41 s, total: 2min 10s\n",
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "forest = forest.fit(train_data_features, train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Making Predictions\n",
    "\n",
    "Doing the same stuff, but with the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.2 s, sys: 1.22 s, total: 24.5 s\n",
      "Wall time: 24.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clean_test_reviews = [review_to_words(review) for review in test.review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.12 s, sys: 141 ms, total: 5.26 s\n",
      "Wall time: 5.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.66 s, sys: 1.47 s, total: 4.13 s\n",
      "Wall time: 5.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = forest.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Creating a Submission\n",
    "# output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "# output.to_csv(\"submissions/Bag_of_Words_model.csv\", \n",
    "#               index = False, \n",
    "#               quoting = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "# Part 2: Word Vectors\n",
    "\n",
    "[source](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords = False):\n",
    "    \"\"\"Function to convert a document to a sequence of words,\n",
    "    optionally removing stop words.\n",
    "    Returns a list of words.\n",
    "    \"\"\"\n",
    "    review_text = BeautifulSoup(review, 'lxml').get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    \n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `punkt` tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_sentences(review, tokenizer, remove_stopwords = False):\n",
    "    \"\"\"Function to split a review into parsed sentences.\n",
    "    Returns a list of sentences, \n",
    "    where each sentence is a list of words\n",
    "    \"\"\"\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    \n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    \n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    \n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 5s, sys: 4.12 s, total: 2min 9s\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentences = []\n",
    "\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.5/site-packages/bs4/__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 4s, sys: 9.12 s, total: 4min 14s\n",
      "Wall time: 4min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Training and Saving Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format = '%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level = logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and train the model.\n",
    "\n",
    "This will take some time: with `num_workers = 4` \n",
    "```\n",
    "CPU times: user 8min 20s, sys: 13.3 s, total: 8min 33s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-11-10 23:49:23,457 : INFO : collecting all words and their counts\n",
      "2016-11-10 23:49:23,462 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2016-11-10 23:49:23,554 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n",
      "2016-11-10 23:49:23,682 : INFO : PROGRESS: at sentence #20000, processed 451867 words, keeping 24947 word types\n",
      "2016-11-10 23:49:23,766 : INFO : PROGRESS: at sentence #30000, processed 671290 words, keeping 30033 word types\n",
      "2016-11-10 23:49:23,834 : INFO : PROGRESS: at sentence #40000, processed 897790 words, keeping 34347 word types\n",
      "2016-11-10 23:49:23,901 : INFO : PROGRESS: at sentence #50000, processed 1116929 words, keeping 37760 word types\n",
      "2016-11-10 23:49:23,969 : INFO : PROGRESS: at sentence #60000, processed 1338370 words, keeping 40722 word types\n",
      "2016-11-10 23:49:24,037 : INFO : PROGRESS: at sentence #70000, processed 1561505 words, keeping 43332 word types\n",
      "2016-11-10 23:49:24,119 : INFO : PROGRESS: at sentence #80000, processed 1780812 words, keeping 45713 word types\n",
      "2016-11-10 23:49:24,188 : INFO : PROGRESS: at sentence #90000, processed 2004905 words, keeping 48134 word types\n",
      "2016-11-10 23:49:24,259 : INFO : PROGRESS: at sentence #100000, processed 2226863 words, keeping 50206 word types\n",
      "2016-11-10 23:49:24,333 : INFO : PROGRESS: at sentence #110000, processed 2446412 words, keeping 52077 word types\n",
      "2016-11-10 23:49:24,401 : INFO : PROGRESS: at sentence #120000, processed 2668549 words, keeping 54113 word types\n",
      "2016-11-10 23:49:24,475 : INFO : PROGRESS: at sentence #130000, processed 2894077 words, keeping 55841 word types\n",
      "2016-11-10 23:49:24,539 : INFO : PROGRESS: at sentence #140000, processed 3106779 words, keeping 57340 word types\n",
      "2016-11-10 23:49:24,611 : INFO : PROGRESS: at sentence #150000, processed 3332313 words, keeping 59049 word types\n",
      "2016-11-10 23:49:24,690 : INFO : PROGRESS: at sentence #160000, processed 3555001 words, keeping 60611 word types\n",
      "2016-11-10 23:49:24,760 : INFO : PROGRESS: at sentence #170000, processed 3778341 words, keeping 62071 word types\n",
      "2016-11-10 23:49:24,833 : INFO : PROGRESS: at sentence #180000, processed 3998922 words, keeping 63490 word types\n",
      "2016-11-10 23:49:24,909 : INFO : PROGRESS: at sentence #190000, processed 4224105 words, keeping 64788 word types\n",
      "2016-11-10 23:49:24,979 : INFO : PROGRESS: at sentence #200000, processed 4448225 words, keeping 66079 word types\n",
      "2016-11-10 23:49:25,049 : INFO : PROGRESS: at sentence #210000, processed 4669555 words, keeping 67383 word types\n",
      "2016-11-10 23:49:25,117 : INFO : PROGRESS: at sentence #220000, processed 4894556 words, keeping 68690 word types\n",
      "2016-11-10 23:49:25,190 : INFO : PROGRESS: at sentence #230000, processed 5117022 words, keeping 69950 word types\n",
      "2016-11-10 23:49:25,259 : INFO : PROGRESS: at sentence #240000, processed 5344527 words, keeping 71159 word types\n",
      "2016-11-10 23:49:25,346 : INFO : PROGRESS: at sentence #250000, processed 5558635 words, keeping 72343 word types\n",
      "2016-11-10 23:49:25,431 : INFO : PROGRESS: at sentence #260000, processed 5778616 words, keeping 73470 word types\n",
      "2016-11-10 23:49:25,498 : INFO : PROGRESS: at sentence #270000, processed 5999905 words, keeping 74759 word types\n",
      "2016-11-10 23:49:25,585 : INFO : PROGRESS: at sentence #280000, processed 6225784 words, keeping 76361 word types\n",
      "2016-11-10 23:49:25,689 : INFO : PROGRESS: at sentence #290000, processed 6448944 words, keeping 77831 word types\n",
      "2016-11-10 23:49:25,787 : INFO : PROGRESS: at sentence #300000, processed 6673547 words, keeping 79163 word types\n",
      "2016-11-10 23:49:25,882 : INFO : PROGRESS: at sentence #310000, processed 6898861 words, keeping 80472 word types\n",
      "2016-11-10 23:49:25,979 : INFO : PROGRESS: at sentence #320000, processed 7123745 words, keeping 81800 word types\n",
      "2016-11-10 23:49:26,086 : INFO : PROGRESS: at sentence #330000, processed 7345404 words, keeping 83022 word types\n",
      "2016-11-10 23:49:26,192 : INFO : PROGRESS: at sentence #340000, processed 7574852 words, keeping 84272 word types\n",
      "2016-11-10 23:49:26,295 : INFO : PROGRESS: at sentence #350000, processed 7798104 words, keeping 85417 word types\n",
      "2016-11-10 23:49:26,398 : INFO : PROGRESS: at sentence #360000, processed 8018679 words, keeping 86587 word types\n",
      "2016-11-10 23:49:26,505 : INFO : PROGRESS: at sentence #370000, processed 8245856 words, keeping 87699 word types\n",
      "2016-11-10 23:49:26,612 : INFO : PROGRESS: at sentence #380000, processed 8470979 words, keeping 88869 word types\n",
      "2016-11-10 23:49:26,713 : INFO : PROGRESS: at sentence #390000, processed 8700551 words, keeping 89893 word types\n",
      "2016-11-10 23:49:26,817 : INFO : PROGRESS: at sentence #400000, processed 8923483 words, keeping 90902 word types\n",
      "2016-11-10 23:49:26,915 : INFO : PROGRESS: at sentence #410000, processed 9144826 words, keeping 91866 word types\n",
      "2016-11-10 23:49:27,018 : INFO : PROGRESS: at sentence #420000, processed 9365858 words, keeping 92899 word types\n",
      "2016-11-10 23:49:27,119 : INFO : PROGRESS: at sentence #430000, processed 9593338 words, keeping 93919 word types\n",
      "2016-11-10 23:49:27,223 : INFO : PROGRESS: at sentence #440000, processed 9820034 words, keeping 94893 word types\n",
      "2016-11-10 23:49:27,325 : INFO : PROGRESS: at sentence #450000, processed 10043796 words, keeping 96024 word types\n",
      "2016-11-10 23:49:27,427 : INFO : PROGRESS: at sentence #460000, processed 10276556 words, keeping 97076 word types\n",
      "2016-11-10 23:49:27,530 : INFO : PROGRESS: at sentence #470000, processed 10504440 words, keeping 97921 word types\n",
      "2016-11-10 23:49:27,629 : INFO : PROGRESS: at sentence #480000, processed 10724824 words, keeping 98850 word types\n",
      "2016-11-10 23:49:27,735 : INFO : PROGRESS: at sentence #490000, processed 10951484 words, keeping 99858 word types\n",
      "2016-11-10 23:49:27,842 : INFO : PROGRESS: at sentence #500000, processed 11173140 words, keeping 100752 word types\n",
      "2016-11-10 23:49:27,946 : INFO : PROGRESS: at sentence #510000, processed 11398393 words, keeping 101685 word types\n",
      "2016-11-10 23:49:28,043 : INFO : PROGRESS: at sentence #520000, processed 11621731 words, keeping 102584 word types\n",
      "2016-11-10 23:49:28,143 : INFO : PROGRESS: at sentence #530000, processed 11846081 words, keeping 103386 word types\n",
      "2016-11-10 23:49:28,250 : INFO : PROGRESS: at sentence #540000, processed 12070696 words, keeping 104251 word types\n",
      "2016-11-10 23:49:28,354 : INFO : PROGRESS: at sentence #550000, processed 12296195 words, keeping 105117 word types\n",
      "2016-11-10 23:49:28,450 : INFO : PROGRESS: at sentence #560000, processed 12517476 words, keeping 105981 word types\n",
      "2016-11-10 23:49:28,579 : INFO : PROGRESS: at sentence #570000, processed 12746461 words, keeping 106771 word types\n",
      "2016-11-10 23:49:28,724 : INFO : PROGRESS: at sentence #580000, processed 12967949 words, keeping 107650 word types\n",
      "2016-11-10 23:49:28,839 : INFO : PROGRESS: at sentence #590000, processed 13193474 words, keeping 108486 word types\n",
      "2016-11-10 23:49:28,929 : INFO : PROGRESS: at sentence #600000, processed 13415660 words, keeping 109203 word types\n",
      "2016-11-10 23:49:29,016 : INFO : PROGRESS: at sentence #610000, processed 13636656 words, keeping 110077 word types\n",
      "2016-11-10 23:49:29,108 : INFO : PROGRESS: at sentence #620000, processed 13862938 words, keeping 110822 word types\n",
      "2016-11-10 23:49:29,194 : INFO : PROGRESS: at sentence #630000, processed 14087222 words, keeping 111596 word types\n",
      "2016-11-10 23:49:29,287 : INFO : PROGRESS: at sentence #640000, processed 14307942 words, keeping 112402 word types\n",
      "2016-11-10 23:49:29,389 : INFO : PROGRESS: at sentence #650000, processed 14533624 words, keeping 113182 word types\n",
      "2016-11-10 23:49:29,491 : INFO : PROGRESS: at sentence #660000, processed 14756308 words, keeping 113932 word types\n",
      "2016-11-10 23:49:29,588 : INFO : PROGRESS: at sentence #670000, processed 14979692 words, keeping 114630 word types\n",
      "2016-11-10 23:49:29,681 : INFO : PROGRESS: at sentence #680000, processed 15204499 words, keeping 115341 word types\n",
      "2016-11-10 23:49:29,791 : INFO : PROGRESS: at sentence #690000, processed 15426666 words, keeping 116118 word types\n",
      "2016-11-10 23:49:29,896 : INFO : PROGRESS: at sentence #700000, processed 15655301 words, keeping 116930 word types\n",
      "2016-11-10 23:49:29,989 : INFO : PROGRESS: at sentence #710000, processed 15878274 words, keeping 117583 word types\n",
      "2016-11-10 23:49:30,090 : INFO : PROGRESS: at sentence #720000, processed 16103516 words, keeping 118207 word types\n",
      "2016-11-10 23:49:30,186 : INFO : PROGRESS: at sentence #730000, processed 16329897 words, keeping 118940 word types\n",
      "2016-11-10 23:49:30,283 : INFO : PROGRESS: at sentence #740000, processed 16550913 words, keeping 119654 word types\n",
      "2016-11-10 23:49:30,387 : INFO : PROGRESS: at sentence #750000, processed 16769240 words, keeping 120282 word types\n",
      "2016-11-10 23:49:30,486 : INFO : PROGRESS: at sentence #760000, processed 16988632 words, keeping 120917 word types\n",
      "2016-11-10 23:49:30,591 : INFO : PROGRESS: at sentence #770000, processed 17215761 words, keeping 121690 word types\n",
      "2016-11-10 23:49:30,693 : INFO : PROGRESS: at sentence #780000, processed 17445902 words, keeping 122389 word types\n",
      "2016-11-10 23:49:30,798 : INFO : PROGRESS: at sentence #790000, processed 17672895 words, keeping 123055 word types\n",
      "2016-11-10 23:49:30,857 : INFO : collected 123493 word types from a corpus of 17795898 raw words and 795538 sentences\n",
      "2016-11-10 23:49:30,858 : INFO : Loading a fresh vocabulary\n",
      "2016-11-10 23:49:31,049 : INFO : min_count=40 retains 16490 unique words (13% of original 123493, drops 107003)\n",
      "2016-11-10 23:49:31,050 : INFO : min_count=40 leaves 17236863 word corpus (96% of original 17795898, drops 559035)\n",
      "2016-11-10 23:49:31,142 : INFO : deleting the raw counts dictionary of 123493 items\n",
      "2016-11-10 23:49:31,153 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2016-11-10 23:49:31,154 : INFO : downsampling leaves estimated 12748070 word corpus (74.0% of prior 17236863)\n",
      "2016-11-10 23:49:31,155 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2016-11-10 23:49:31,228 : INFO : resetting layer weights\n",
      "2016-11-10 23:49:31,677 : INFO : training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2016-11-10 23:49:31,678 : INFO : expecting 795538 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-11-10 23:49:32,704 : INFO : PROGRESS: at 0.68% examples, 435100 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:49:33,707 : INFO : PROGRESS: at 1.44% examples, 456027 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:49:34,712 : INFO : PROGRESS: at 2.16% examples, 455768 words/s, in_qsize 8, out_qsize 1\n",
      "2016-11-10 23:49:35,718 : INFO : PROGRESS: at 2.92% examples, 461117 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:49:36,721 : INFO : PROGRESS: at 3.55% examples, 447375 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:49:37,740 : INFO : PROGRESS: at 4.12% examples, 432220 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:49:38,743 : INFO : PROGRESS: at 4.66% examples, 419457 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:49:39,806 : INFO : PROGRESS: at 5.10% examples, 398822 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:49:40,839 : INFO : PROGRESS: at 5.60% examples, 388124 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:49:41,893 : INFO : PROGRESS: at 6.05% examples, 376729 words/s, in_qsize 7, out_qsize 1\n",
      "2016-11-10 23:49:42,895 : INFO : PROGRESS: at 6.39% examples, 361519 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:49:43,904 : INFO : PROGRESS: at 6.88% examples, 356823 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:49:44,958 : INFO : PROGRESS: at 7.48% examples, 357570 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 23:49:45,966 : INFO : PROGRESS: at 8.04% examples, 357387 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:49:46,971 : INFO : PROGRESS: at 8.46% examples, 351660 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 23:49:47,980 : INFO : PROGRESS: at 8.85% examples, 345242 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:49:49,000 : INFO : PROGRESS: at 9.24% examples, 339333 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:49:50,009 : INFO : PROGRESS: at 9.87% examples, 342516 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:49:51,013 : INFO : PROGRESS: at 10.52% examples, 346166 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:49:52,022 : INFO : PROGRESS: at 11.25% examples, 352190 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:49:53,027 : INFO : PROGRESS: at 11.95% examples, 356722 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:49:54,035 : INFO : PROGRESS: at 12.67% examples, 361106 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 23:49:55,040 : INFO : PROGRESS: at 13.35% examples, 364280 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:49:56,071 : INFO : PROGRESS: at 14.11% examples, 368832 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:49:57,076 : INFO : PROGRESS: at 14.85% examples, 372802 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:49:58,084 : INFO : PROGRESS: at 15.57% examples, 375911 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:49:59,092 : INFO : PROGRESS: at 16.28% examples, 378529 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 23:50:00,097 : INFO : PROGRESS: at 17.01% examples, 381737 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:01,099 : INFO : PROGRESS: at 17.72% examples, 384046 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:02,105 : INFO : PROGRESS: at 18.35% examples, 384751 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:03,139 : INFO : PROGRESS: at 19.00% examples, 385057 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:04,152 : INFO : PROGRESS: at 19.52% examples, 383387 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:05,175 : INFO : PROGRESS: at 20.11% examples, 383002 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:06,191 : INFO : PROGRESS: at 20.78% examples, 383937 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:07,202 : INFO : PROGRESS: at 21.47% examples, 385490 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:50:08,230 : INFO : PROGRESS: at 22.23% examples, 387779 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:09,240 : INFO : PROGRESS: at 23.00% examples, 390297 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:10,259 : INFO : PROGRESS: at 23.68% examples, 391093 words/s, in_qsize 8, out_qsize 1\n",
      "2016-11-10 23:50:11,278 : INFO : PROGRESS: at 24.45% examples, 393321 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 23:50:12,287 : INFO : PROGRESS: at 25.18% examples, 394990 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 23:50:13,293 : INFO : PROGRESS: at 25.86% examples, 395917 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:50:14,301 : INFO : PROGRESS: at 26.64% examples, 397947 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:15,309 : INFO : PROGRESS: at 27.39% examples, 399723 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:16,311 : INFO : PROGRESS: at 28.16% examples, 401807 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:17,313 : INFO : PROGRESS: at 28.94% examples, 403795 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:18,315 : INFO : PROGRESS: at 29.71% examples, 405841 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:19,325 : INFO : PROGRESS: at 30.49% examples, 407599 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:50:20,327 : INFO : PROGRESS: at 31.25% examples, 409333 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:50:21,339 : INFO : PROGRESS: at 32.01% examples, 410927 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:50:22,342 : INFO : PROGRESS: at 32.78% examples, 412397 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:23,367 : INFO : PROGRESS: at 33.45% examples, 412531 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:24,380 : INFO : PROGRESS: at 34.23% examples, 414106 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:25,399 : INFO : PROGRESS: at 35.02% examples, 415573 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:50:26,400 : INFO : PROGRESS: at 35.74% examples, 416334 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:50:27,420 : INFO : PROGRESS: at 36.46% examples, 416926 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:50:28,438 : INFO : PROGRESS: at 37.24% examples, 418281 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:29,444 : INFO : PROGRESS: at 38.01% examples, 419531 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:30,484 : INFO : PROGRESS: at 38.78% examples, 420507 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:31,486 : INFO : PROGRESS: at 39.50% examples, 421100 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:50:32,492 : INFO : PROGRESS: at 40.23% examples, 421900 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 23:50:33,495 : INFO : PROGRESS: at 40.93% examples, 422232 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:34,509 : INFO : PROGRESS: at 41.66% examples, 422687 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:35,524 : INFO : PROGRESS: at 42.36% examples, 422920 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 23:50:36,553 : INFO : PROGRESS: at 43.10% examples, 423495 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:37,555 : INFO : PROGRESS: at 43.81% examples, 423781 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:38,560 : INFO : PROGRESS: at 44.54% examples, 424374 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:50:39,562 : INFO : PROGRESS: at 45.32% examples, 425372 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 23:50:40,579 : INFO : PROGRESS: at 46.01% examples, 425524 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:50:41,583 : INFO : PROGRESS: at 46.71% examples, 425636 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:50:42,599 : INFO : PROGRESS: at 47.37% examples, 425487 words/s, in_qsize 7, out_qsize 1\n",
      "2016-11-10 23:50:43,616 : INFO : PROGRESS: at 48.12% examples, 426134 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:50:44,622 : INFO : PROGRESS: at 48.87% examples, 426821 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:45,626 : INFO : PROGRESS: at 49.61% examples, 427497 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:46,653 : INFO : PROGRESS: at 50.37% examples, 428136 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:50:47,666 : INFO : PROGRESS: at 51.08% examples, 428350 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:48,674 : INFO : PROGRESS: at 51.77% examples, 428589 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:50:49,682 : INFO : PROGRESS: at 52.49% examples, 428920 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:50,701 : INFO : PROGRESS: at 53.13% examples, 428550 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:51,702 : INFO : PROGRESS: at 53.79% examples, 428457 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:52,738 : INFO : PROGRESS: at 54.44% examples, 428187 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:50:53,757 : INFO : PROGRESS: at 55.13% examples, 428194 words/s, in_qsize 7, out_qsize 1\n",
      "2016-11-10 23:50:54,778 : INFO : PROGRESS: at 55.85% examples, 428449 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:50:55,780 : INFO : PROGRESS: at 56.51% examples, 428349 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:50:56,781 : INFO : PROGRESS: at 57.22% examples, 428609 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:50:57,789 : INFO : PROGRESS: at 57.82% examples, 428070 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 23:50:58,809 : INFO : PROGRESS: at 58.54% examples, 428312 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 23:50:59,829 : INFO : PROGRESS: at 59.31% examples, 428869 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:00,833 : INFO : PROGRESS: at 60.03% examples, 429331 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:01,868 : INFO : PROGRESS: at 60.75% examples, 429478 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:02,871 : INFO : PROGRESS: at 61.43% examples, 429448 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:51:03,884 : INFO : PROGRESS: at 62.13% examples, 429544 words/s, in_qsize 8, out_qsize 1\n",
      "2016-11-10 23:51:04,891 : INFO : PROGRESS: at 62.92% examples, 430283 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:05,895 : INFO : PROGRESS: at 63.70% examples, 430858 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:51:06,903 : INFO : PROGRESS: at 64.40% examples, 430954 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 23:51:07,903 : INFO : PROGRESS: at 65.14% examples, 431378 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:08,905 : INFO : PROGRESS: at 65.91% examples, 432005 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:51:09,924 : INFO : PROGRESS: at 66.72% examples, 432682 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:10,927 : INFO : PROGRESS: at 67.50% examples, 433352 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:11,935 : INFO : PROGRESS: at 68.25% examples, 433776 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:51:12,935 : INFO : PROGRESS: at 69.02% examples, 434369 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 23:51:13,944 : INFO : PROGRESS: at 69.80% examples, 434975 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:15,018 : INFO : PROGRESS: at 70.31% examples, 433594 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:16,022 : INFO : PROGRESS: at 70.89% examples, 432959 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:17,053 : INFO : PROGRESS: at 71.48% examples, 432380 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:18,060 : INFO : PROGRESS: at 72.20% examples, 432654 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:19,095 : INFO : PROGRESS: at 72.82% examples, 432145 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 23:51:20,116 : INFO : PROGRESS: at 73.54% examples, 432298 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:21,122 : INFO : PROGRESS: at 74.30% examples, 432771 words/s, in_qsize 8, out_qsize 1\n",
      "2016-11-10 23:51:22,132 : INFO : PROGRESS: at 75.07% examples, 433282 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:51:23,141 : INFO : PROGRESS: at 75.81% examples, 433602 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 23:51:24,141 : INFO : PROGRESS: at 76.60% examples, 434192 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:25,154 : INFO : PROGRESS: at 77.37% examples, 434663 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 23:51:26,166 : INFO : PROGRESS: at 78.14% examples, 435132 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:51:27,171 : INFO : PROGRESS: at 78.92% examples, 435615 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:28,186 : INFO : PROGRESS: at 79.69% examples, 436048 words/s, in_qsize 6, out_qsize 1\n",
      "2016-11-10 23:51:29,187 : INFO : PROGRESS: at 80.48% examples, 436653 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:30,187 : INFO : PROGRESS: at 81.26% examples, 437130 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:31,188 : INFO : PROGRESS: at 82.04% examples, 437597 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:32,197 : INFO : PROGRESS: at 82.82% examples, 438031 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:33,198 : INFO : PROGRESS: at 83.62% examples, 438538 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:34,221 : INFO : PROGRESS: at 84.41% examples, 438961 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:35,223 : INFO : PROGRESS: at 85.18% examples, 439397 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:51:36,241 : INFO : PROGRESS: at 85.94% examples, 439708 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:51:37,251 : INFO : PROGRESS: at 86.73% examples, 440090 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:38,262 : INFO : PROGRESS: at 87.51% examples, 440529 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:39,272 : INFO : PROGRESS: at 88.29% examples, 440904 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:51:40,275 : INFO : PROGRESS: at 89.07% examples, 441356 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:41,283 : INFO : PROGRESS: at 89.81% examples, 441623 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:42,288 : INFO : PROGRESS: at 90.57% examples, 441896 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:51:43,303 : INFO : PROGRESS: at 91.35% examples, 442340 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:44,303 : INFO : PROGRESS: at 92.12% examples, 442720 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:45,316 : INFO : PROGRESS: at 92.90% examples, 443115 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:46,327 : INFO : PROGRESS: at 93.68% examples, 443449 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:51:47,336 : INFO : PROGRESS: at 94.44% examples, 443787 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:48,339 : INFO : PROGRESS: at 95.23% examples, 444197 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:49,362 : INFO : PROGRESS: at 95.89% examples, 443964 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:51:50,363 : INFO : PROGRESS: at 96.42% examples, 443181 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:51,371 : INFO : PROGRESS: at 97.01% examples, 442699 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-10 23:51:52,373 : INFO : PROGRESS: at 97.77% examples, 442998 words/s, in_qsize 7, out_qsize 1\n",
      "2016-11-10 23:51:53,405 : INFO : PROGRESS: at 98.55% examples, 443252 words/s, in_qsize 7, out_qsize 3\n",
      "2016-11-10 23:51:54,420 : INFO : PROGRESS: at 99.36% examples, 443701 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-10 23:51:55,322 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2016-11-10 23:51:55,342 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-11-10 23:51:55,355 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-11-10 23:51:55,369 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-11-10 23:51:55,370 : INFO : training on 88979490 raw words (63740053 effective words) took 143.7s, 443649 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 2s, sys: 9.51 s, total: 8min 12s\n",
      "Wall time: 2min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = word2vec.Word2Vec(sentences, \n",
    "                          workers = num_workers, \n",
    "                          size = num_features, \n",
    "                          min_count = min_word_count,\n",
    "                          window = context, \n",
    "                          sample = downsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't plan to train the model any further, calling `init_sims` will make the model much more memory-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-11-10 23:51:55,401 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "model.init_sims(replace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be helpful to create a meaningful model name and save the model for later use. You can load it later using `Word2Vec.load()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-11-10 23:51:55,700 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2016-11-10 23:51:55,703 : INFO : not storing attribute cum_table\n",
      "2016-11-10 23:51:55,705 : INFO : not storing attribute syn0norm\n",
      "2016-11-10 23:51:56,588 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6225396394729614),\n",
       " ('lady', 0.6021609306335449),\n",
       " ('lad', 0.6000834107398987),\n",
       " ('monk', 0.5472439527511597),\n",
       " ('guy', 0.5301573276519775),\n",
       " ('chap', 0.528995156288147),\n",
       " ('soldier', 0.5207506418228149),\n",
       " ('farmer', 0.5195120573043823),\n",
       " ('men', 0.5185949802398682),\n",
       " ('businessman', 0.5123093128204346)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.6707057952880859),\n",
       " ('bride', 0.6562491059303284),\n",
       " ('duchess', 0.624799370765686),\n",
       " ('victoria', 0.6214991807937622),\n",
       " ('stepmother', 0.6054560542106628),\n",
       " ('maid', 0.6039066314697266),\n",
       " ('mistress', 0.6025797128677368),\n",
       " ('latifah', 0.5982476472854614),\n",
       " ('eva', 0.5889024138450623),\n",
       " ('goddess', 0.5796060562133789)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7803564667701721),\n",
       " ('horrible', 0.7395068407058716),\n",
       " ('atrocious', 0.7191551923751831),\n",
       " ('dreadful', 0.7096336483955383),\n",
       " ('abysmal', 0.7004678249359131),\n",
       " ('horrendous', 0.6907781958580017),\n",
       " ('appalling', 0.6566586494445801),\n",
       " ('horrid', 0.6554067134857178),\n",
       " ('lousy', 0.6236556768417358),\n",
       " ('bad', 0.6007861495018005)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Part 3: More Fun With Word Vectors\n",
    "\n",
    "[source](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors)\n",
    "\n",
    "## Numeric Representations of Words\n",
    "\n",
    "The Word2Vec model trained in Part 2 consists of a feature vector for each word in the vocabulary, stored in a `numpy` array called \"`syn0`\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-11-10 23:51:56,722 : INFO : loading Word2Vec object from 300features_40minwords_10context\n",
      "2016-11-10 23:51:57,390 : INFO : setting ignored attribute cum_table to None\n",
      "2016-11-10 23:51:57,392 : INFO : setting ignored attribute syn0norm to None\n",
      "2016-11-10 23:51:57,394 : INFO : loaded 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16490, 300)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.syn0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows in `syn0` is the number of words in the model's vocabulary, and the number of columns corresponds to the size of the feature vector, which we set in Part 2.  Setting the minimum word count to 40 gave us a total vocabulary of 16,492 words with 300 features apiece. Individual word vectors can be accessed in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model[\"flower\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## From Words To Paragraphs, Attempt 1: Vector Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"Function to average all of the word vectors in a given\n",
    "    paragraph\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,), dtype = \"float64\")\n",
    "    nwords = 0.\n",
    "\n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    " \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    \"\"\"Given a set of reviews (each one a list of words), \n",
    "    calculate the average feature vector \n",
    "    for each one and return a 2D numpy array \n",
    "    \"\"\"\n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features), dtype = \"float64\")\n",
    "    \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        # Print a status message every 1000th review\n",
    "        if counter % 1000. == 0.:\n",
    "            (\"Review %d of %d\" % (counter, len(reviews)))\n",
    "        \n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        # Increment the counter\n",
    "        counter = counter + 1.\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate average feature vectors for training and testing sets,\n",
    "using the functions we defined above. Notice that we now use stop word\n",
    "removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:22: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 34s, sys: 5.99 s, total: 1min 40s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clean_train_reviews = []\n",
    "\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append(review_to_wordlist(review, \n",
    "                                                  remove_stopwords = True))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:22: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 35s, sys: 2.75 s, total: 1min 38s\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clean_test_reviews = []\n",
    "\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Next, use the average paragraph vectors to train a random forest. Note that, as in Part 1, we can only use the labeled training reviews to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 1e+03 ns, total: 8 µs\n",
      "Wall time: 15 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# forest = forest.fit(trainDataVecs, train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I get an error here:\n",
    "```\n",
    "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# result = forest.predict(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Write the test results \n",
    "# output = pd.DataFrame(data = {\"id\":test[\"id\"], \"sentiment\":result})\n",
    "# output.to_csv(\"Word2Vec_AverageVectors.csv\", \n",
    "#               index = False, \n",
    "#               quoting = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## From Words to Paragraphs, Attempt 2: Clustering \n",
    "\n",
    "Word2Vec creates clusters of semantically related words, so another possible approach is to exploit the similarity of words within a cluster. Grouping vectors in this way is known as \"vector quantization.\" To accomplish this, we first need to find the centers of the word clusters, which we can do by using a clustering algorithm such as K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set \"k\" (`num_clusters`) to be 1/5th of the vocabulary size, or an average of 5 words per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectors = model.syn0\n",
    "num_clusters = word_vectors.shape[0] // 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans_clustering = KMeans(n_clusters = num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 6s, sys: 52.1 s, total: 15min 58s\n",
      "Wall time: 11min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Word / Index dictionary, mapping each vocabulary word to a cluster number                                                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_centroid_map = dict(zip( model.index2word, idx ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little abstract, so let's take a closer look at what our clusters contain. Your clusters may differ, as Word2Vec relies on a random number seed. Here is a loop that prints out the words for clusters 0 through 9:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['sledgehammer']\n",
      "\n",
      "Cluster 1\n",
      "['janis', 'joplin', 'harp', 'hendrix', 'mozart']\n",
      "\n",
      "Cluster 2\n",
      "['demonic', 'slugs', 'humanoid', 'insects', 'rotting', 'wasps', 'spiders', 'eaters', 'mutated', 'prehistoric', 'toxic', 'rats', 'worms', 'ants', 'raptor', 'infested', 'pollution', 'radioactive']\n",
      "\n",
      "Cluster 3\n",
      "['discovery']\n",
      "\n",
      "Cluster 4\n",
      "['celebration', 'monastery', 'temporary', 'retreat']\n",
      "\n",
      "Cluster 5\n",
      "['custody', 'injured', 'appointed', 'wounded', 'healed', 'slave', 'imprisoned', 'courts', 'condemned', 'freed', 'starving']\n",
      "\n",
      "Cluster 6\n",
      "['harrison', 'hopkins', 'jeff', 'lloyd', 'john', 'trevor', 'donald', 'alan', 'richard']\n",
      "\n",
      "Cluster 7\n",
      "['downbeat', 'anticlimactic', 'hasty']\n",
      "\n",
      "Cluster 8\n",
      "['genius', 'greatness', 'mastery', 'brilliance', 'excellence']\n",
      "\n",
      "Cluster 9\n",
      "['jeep', 'getaway', 'crossing', 'car', 'blast', 'truck', 'passenger', 'train', 'helicopter', 'bus']\n"
     ]
    }
   ],
   "source": [
    "for cluster in range(10):\n",
    "    print(\"\\nCluster %d\" % cluster)\n",
    "\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    for i in range(len(word_centroid_map.values())):\n",
    "        if( list(word_centroid_map.values())[i] == cluster ):\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a cluster (or \"centroid\") assignment for each word, and we can define a function to convert reviews into bags-of-centroids. This works just like Bag of Words but uses semantically related clusters instead of individual words.\n",
    "\n",
    "* The number of clusters is equal to the highest cluster index in the word / centroid map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(wordlist, word_centroid_map):\n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    \n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    \n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    \n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the training set reviews into bags of centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.1 s, sys: 1.41 s, total: 44.5 s\n",
      "Wall time: 46.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_centroids = np.zeros((train[\"review\"].size, num_clusters),\n",
    "                           dtype = \"float32\")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for test reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.3 s, sys: 1.35 s, total: 44.7 s\n",
      "Wall time: 46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_centroids = np.zeros((test[\"review\"].size, num_clusters), \n",
    "                          dtype = \"float32\")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Fit a random forest and extract predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 32s, sys: 1.76 s, total: 1min 34s\n",
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "forest = forest.fit(train_centroids, train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.14 s, sys: 450 ms, total: 2.59 s\n",
      "Wall time: 3.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = forest.predict(test_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Write the test results \n",
    "# output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "# output.to_csv(\"BagOfCentroids.csv\", \n",
    "#               index = False, \n",
    "#               quoting = 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
